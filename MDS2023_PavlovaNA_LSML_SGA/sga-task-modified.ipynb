{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# User routes on the site\n",
    "## Description\n",
    "**Clickstream** is a sequence of user actions on a website. It allows you to understand how users interact with the site. In this task, you need to find the most frequent custom routes.\n",
    "\n",
    "## Input data\n",
    "Input data is а table with clickstream data in file `hdfs:/data/clickstream.csv`.\n",
    "\n",
    "### Table structure\n",
    "* `user_id (int)` - Unique user identifier.\n",
    "* `session_id (int)` - Unique identifier for the user session. The user's session lasts until the identifier changes.\n",
    "* `event_type (string)` - Event type from the list:\n",
    "    * **page** - visit to the page\n",
    "    * **event** - any action on the page\n",
    "    * <b>&lt;custom&gt;</b> - string with any other type\n",
    "* `event_type (string)` - Page on the site.\n",
    "* `timestamp (int)` - Unix-timestamp of action.\n",
    "\n",
    "### Browser errors\n",
    "Errors can sometimes occur in the user's browser - after such an error appears, we can no longer trust the data of this session and all the following lines after the error or at the same time with it are considered corrupted and **should not be counted** in statistics.\n",
    "\n",
    "When an error occurs on the page, a random string containing the word **error** will be written to the `event_type` field.\n",
    "\n",
    "### Sample of user session\n",
    "<pre>\n",
    "+-------+----------+------------+----------+----------+\n",
    "|user_id|session_id|  event_type|event_page| timestamp|\n",
    "+-------+----------+------------+----------+----------+\n",
    "|    562|       507|        page|      main|1620494781|\n",
    "|    562|       507|       event|      main|1620494788|\n",
    "|    562|       507|       event|      main|1620494798|\n",
    "|    562|       507|        page|    family|1620494820|\n",
    "|    562|       507|       event|    family|1620494828|\n",
    "|    562|       507|        page|      main|1620494848|\n",
    "|    562|       507|wNaxLlerrorU|      main|1620494865|\n",
    "|    562|       507|       event|      main|1620494873|\n",
    "|    562|       507|        page|      news|1620494875|\n",
    "|    562|       507|        page|   tariffs|1620494876|\n",
    "|    562|       507|       event|   tariffs|1620494884|\n",
    "|    562|       514|        page|      main|1620728918|\n",
    "|    562|       514|       event|      main|1620729174|\n",
    "|    562|       514|        page|   archive|1620729674|\n",
    "|    562|       514|        page|     bonus|1620729797|\n",
    "|    562|       514|        page|   tariffs|1620731090|\n",
    "|    562|       514|       event|   tariffs|1620731187|\n",
    "+-------+----------+------------+----------+----------+\n",
    "</pre>\n",
    "\n",
    "#### Correct user routes for a given user:\n",
    "* **Session 507**: main-family-main\n",
    "* **Session 514**: main-archive-bonus-tariffs\n",
    "\n",
    "Route elements are ordered by the time they appear in the clickstream, from earliest to latest.\n",
    "\n",
    "The route must be accounted for completely before the end of the session or an error in the session.\n",
    "\n",
    "## Task\n",
    "You need to use the Spark SQL, Spark RDD and Spark DF interfaces to create a solution file, the lines of which contain **the 30 most frequent user routes** on the site.\n",
    "\n",
    "Each line of the file should contain the `route` and `count` values **separated by tabs**, where:\n",
    "* `route` - route on the site, consisting of pages separated by \"-\".\n",
    "* `count` - the number of user sessions in which this route was.\n",
    "\n",
    "The lines must be **ordered in descending order** of the `count` field.\n",
    "\n",
    "## Criteria\n",
    "You can get maximum of 3.5 points (final grade) for this assignment, depedning on the number of interface you manage to leverage. The criteria are as follows:\n",
    "\n",
    "* 0.5 points – Spark SQL solution with 1 query\n",
    "* 0.5 points – Spark SQL solution with <=2 queries\n",
    "* 0.5 points – Spark RDD solution\n",
    "* 0.5 points – Spark DF solution\n",
    "* 0.5 points – your solution algorithm is relatively optimized, i.e.: no O^2 or O^3 complexities; appropriate object usage; no data leaks etc. This is evaluated by staff.\n",
    "* 1 point – 1 on 1 screening session. During this session staff member can ask you questions regarding your solution logic, framework usage, questionable parts of your code etc. If your code is clean enough, the staff member can just ask you to solve a theoretical problem connected to Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.7\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840633 sha256=c30393c78cffefc78c39ddfd4cbc827409312574d9e5852cfaff872fd5b75436\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.3\n"
     ]
    }
   ],
   "source": [
    "! pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `/data': File exists\n",
      "copyFromLocal: `/data/clickstream.csv': File exists\n",
      "Found 1 items\n",
      "-rw-r--r--   1 jovyan supergroup     30.7 M 2024-10-23 20:13 /data/clickstream.csv\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -mkdir /data/\n",
    "! hadoop fs -copyFromLocal clickstream.csv /data/\n",
    "! hadoop fs -ls -h /data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2024-11-08 21:52:17,131 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName='jupyter')\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "se = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------+----------+----------+\n",
      "|user_id|session_id|  event_type|event_page| timestamp|\n",
      "+-------+----------+------------+----------+----------+\n",
      "|    562|       507|        page|      main|1695584127|\n",
      "|    562|       507|       event|      main|1695584134|\n",
      "|    562|       507|       event|      main|1695584144|\n",
      "|    562|       507|       event|      main|1695584147|\n",
      "|    562|       507|wNaxLlerrorU|      main|1695584154|\n",
      "+-------+----------+------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = se.read.format(\"csv\").option(\"header\", \"true\").option(\"sep\", \"\\t\").load(\"hdfs:/data/clickstream.csv\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Mixed solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---+--------------------+\n",
      "|user_id|session_id|cnt|               route|\n",
      "+-------+----------+---+--------------------+\n",
      "|      0|       874|  3|  main-rabota-online|\n",
      "|      0|       888|  1|                main|\n",
      "|      0|       907|  6|main-digital-news...|\n",
      "|      1|      1013|  4|main-tariffs-onli...|\n",
      "|      1|      1026|  1|                main|\n",
      "|      1|      1030|  2|        main-archive|\n",
      "|      1|      1036|  1|                main|\n",
      "|      1|      1038|  1|                main|\n",
      "|      1|       963|  2|           main-news|\n",
      "|      1|       979|  5|main-rabota-archi...|\n",
      "|      1|       992|  5|main-rabota-inter...|\n",
      "|     10|       730| 10|main-archive-tari...|\n",
      "|     10|       753|  2|         main-rabota|\n",
      "|     10|       762| 12|main-internet-arc...|\n",
      "|    100|       937| 15|main-archive-onli...|\n",
      "|    100|       939|  3|  main-rabota-online|\n",
      "|    100|       984|  6|main-internet-tar...|\n",
      "|   1000|        86|  1|                main|\n",
      "|   1000|        91|  2|        main-archive|\n",
      "|   1001|        11|  4|main-bonus-intern...|\n",
      "+-------+----------+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.registerTempTable(\"click\")\n",
    "\n",
    "routes_df = se.sql(\"\"\"\n",
    "SELECT user_id, session_id, COUNT(*) AS cnt, \n",
    "       CONCAT_WS('-', COLLECT_LIST(event_page)) AS route\n",
    "FROM (\n",
    "    SELECT user_id, session_id, event_page\n",
    "    FROM (SELECT user_id, session_id, event_page, no\n",
    "FROM (\n",
    "    SELECT user_id, session_id, event_page, no,\n",
    "           LAG(event_page, 1, NULL) OVER (PARTITION BY user_id, session_id ORDER BY no) AS prev_event\n",
    "    FROM \n",
    "    (SELECT user_id, session_id, event_page, \n",
    "       ROW_NUMBER() OVER (PARTITION BY user_id, session_id ORDER BY timestamp ASC) AS no\n",
    "FROM (\n",
    "    SELECT *, \n",
    "           MAX(CASE WHEN event_type NOT IN ('page', 'event') THEN 1 ELSE 0 END) OVER (PARTITION BY user_id, session_id ORDER BY timestamp ASC) AS error_tag\n",
    "    FROM click\n",
    ") filtered\n",
    "WHERE error_tag = 0\n",
    "ORDER BY user_id, session_id, no) as grp\n",
    ") filtered\n",
    "WHERE event_page != prev_event OR prev_event IS NULL\n",
    "ORDER BY user_id, session_id, no) as wo_dup\n",
    "    ORDER BY user_id, session_id, no\n",
    ") sorted_events\n",
    "GROUP BY user_id, session_id\n",
    "\"\"\")\n",
    "\n",
    "routes_df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "routes_rdd = routes_df.rdd.map(lambda row: (row['route'], 1)) \\\n",
    "                          .reduceByKey(lambda a, b: a + b) \\\n",
    "                          .sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "top_routes = routes_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_30 = routes_rdd.take(30)\n",
    "top_30_tsv = [ x[0]+\"\\t\"+str(x[1]) for x in top_30]\n",
    "output_path = \"top_30_routes.tsv\"\n",
    "f = open(output_path, \"w\")\n",
    "for line in top_30_tsv:\n",
    "    f.write(line+\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'main': 8184, 'main-archive': 1113, 'main-rabota': 1047, 'main-internet': 897, 'main-bonus': 870, 'main-news': 769, 'main-tariffs': 677, 'main-online': 587, 'main-vklad': 518, 'main-rabota-archive': 170}\n",
      "Top 10 routes saved to result.json\n"
     ]
    }
   ],
   "source": [
    "result_dict = {route: count for route, count in top_routes}\n",
    "output_path = \"result.json\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(result_dict, f)\n",
    "\n",
    "print(result_dict)\n",
    "print(f\"Top 10 routes saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Spark SQL Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------+----------+----------+\n",
      "|user_id|session_id|  event_type|event_page| timestamp|\n",
      "+-------+----------+------------+----------+----------+\n",
      "|    562|       507|        page|      main|1695584127|\n",
      "|    562|       507|       event|      main|1695584134|\n",
      "|    562|       507|       event|      main|1695584144|\n",
      "|    562|       507|       event|      main|1695584147|\n",
      "|    562|       507|wNaxLlerrorU|      main|1695584154|\n",
      "+-------+----------+------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = se.read.format(\"csv\").option(\"header\", \"true\").option(\"sep\", \"\\t\").load(\"hdfs:/data/clickstream.csv\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               route|count|\n",
      "+--------------------+-----+\n",
      "|                main| 8184|\n",
      "|        main-archive| 1113|\n",
      "|         main-rabota| 1047|\n",
      "|       main-internet|  897|\n",
      "|          main-bonus|  870|\n",
      "|           main-news|  769|\n",
      "|        main-tariffs|  677|\n",
      "|         main-online|  587|\n",
      "|          main-vklad|  518|\n",
      "| main-rabota-archive|  170|\n",
      "| main-archive-rabota|  167|\n",
      "|  main-bonus-archive|  143|\n",
      "|   main-rabota-bonus|  139|\n",
      "|    main-news-rabota|  135|\n",
      "|   main-bonus-rabota|  135|\n",
      "|main-archive-inte...|  132|\n",
      "|    main-rabota-news|  130|\n",
      "|main-internet-rabota|  129|\n",
      "|   main-archive-news|  126|\n",
      "|main-rabota-internet|  124|\n",
      "|main-internet-arc...|  122|\n",
      "|  main-archive-bonus|  117|\n",
      "| main-internet-bonus|  115|\n",
      "|main-tariffs-inte...|  114|\n",
      "|   main-news-archive|  113|\n",
      "|  main-news-internet|  109|\n",
      "|main-archive-tariffs|  104|\n",
      "|  main-internet-news|  103|\n",
      "|main-tariffs-archive|  103|\n",
      "|    main-rabota-main|   94|\n",
      "+--------------------+-----+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.registerTempTable(\"click_1\")\n",
    "\n",
    "routes_df = se.sql(\"\"\"\n",
    "SELECT route, COUNT(*) as count\n",
    "FROM (SELECT user_id, session_id, COUNT(*) AS cnt, \n",
    "           CONCAT_WS('-', COLLECT_LIST(event_page)) AS route\n",
    "    FROM (\n",
    "        SELECT user_id, session_id, event_page\n",
    "        FROM (SELECT user_id, session_id, event_page, no\n",
    "    FROM (\n",
    "        SELECT user_id, session_id, event_page, no,\n",
    "               LAG(event_page, 1, NULL) OVER (PARTITION BY user_id, session_id ORDER BY no) AS prev_event\n",
    "        FROM \n",
    "        (SELECT user_id, session_id, event_page, \n",
    "           ROW_NUMBER() OVER (PARTITION BY user_id, session_id ORDER BY timestamp ASC) AS no\n",
    "    FROM (\n",
    "        SELECT *, \n",
    "               MAX(CASE WHEN event_type NOT IN ('page', 'event') THEN 1 ELSE 0 END) OVER (PARTITION BY user_id, session_id ORDER BY timestamp ASC) AS error_tag\n",
    "        FROM click_1\n",
    "    ) filtered\n",
    "    WHERE error_tag = 0\n",
    "    ORDER BY user_id, session_id, no) as grp\n",
    "    ) filtered\n",
    "    WHERE event_page != prev_event OR prev_event IS NULL\n",
    "    ORDER BY user_id, session_id, no) as wo_dup\n",
    "        ORDER BY user_id, session_id, no\n",
    "    ) sorted_events\n",
    "    GROUP BY user_id, session_id)\n",
    "GROUP BY route\n",
    "ORDER BY count DESC\n",
    "\"\"\")\n",
    "\n",
    "routes_df.show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Spark DF solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------+----------+----------+\n",
      "|user_id|session_id|  event_type|event_page| timestamp|\n",
      "+-------+----------+------------+----------+----------+\n",
      "|    562|       507|        page|      main|1695584127|\n",
      "|    562|       507|       event|      main|1695584134|\n",
      "|    562|       507|       event|      main|1695584144|\n",
      "|    562|       507|       event|      main|1695584147|\n",
      "|    562|       507|wNaxLlerrorU|      main|1695584154|\n",
      "+-------+----------+------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = se.read.format(\"csv\").option(\"header\", \"true\").option(\"sep\", \"\\t\").load(\"hdfs:/data/clickstream.csv\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+----------+----------+---------+\n",
      "|user_id|session_id|event_type|event_page| timestamp|error_tag|\n",
      "+-------+----------+----------+----------+----------+---------+\n",
      "|      1|      1026|      page|      main|1700148765|        0|\n",
      "|     10|       762|      page|      main|1699442253|        0|\n",
      "+-------+----------+----------+----------+----------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+----------+\n",
      "|user_id|session_id|event_page| timestamp|\n",
      "+-------+----------+----------+----------+\n",
      "|      0|       874|      main|1696372696|\n",
      "|      0|       874|      main|1696371064|\n",
      "+-------+----------+----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+---+\n",
      "|user_id|session_id|event_page| no|\n",
      "+-------+----------+----------+---+\n",
      "|      0|       874|      main|  1|\n",
      "|      0|       874|      main|  2|\n",
      "+-------+----------+----------+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+---+----------+\n",
      "|user_id|session_id|event_page| no|prev_event|\n",
      "+-------+----------+----------+---+----------+\n",
      "|      0|       874|      main|  1|      null|\n",
      "|      0|       874|      main|  2|      main|\n",
      "+-------+----------+----------+---+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+---+\n",
      "|user_id|session_id|event_page| no|\n",
      "+-------+----------+----------+---+\n",
      "|      0|       874|      main|  1|\n",
      "|      0|       874|    rabota|  4|\n",
      "+-------+----------+----------+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------+\n",
      "|user_id|session_id|               route|\n",
      "+-------+----------+--------------------+\n",
      "|      0|       874|  main-rabota-online|\n",
      "|      0|       879|main-online-tarif...|\n",
      "|      0|       885|                main|\n",
      "+-------+----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 740:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               route|count|\n",
      "+--------------------+-----+\n",
      "|                main| 8184|\n",
      "|        main-archive| 1113|\n",
      "|         main-rabota| 1047|\n",
      "|       main-internet|  897|\n",
      "|          main-bonus|  870|\n",
      "|           main-news|  769|\n",
      "|        main-tariffs|  677|\n",
      "|         main-online|  587|\n",
      "|          main-vklad|  518|\n",
      "| main-rabota-archive|  170|\n",
      "| main-archive-rabota|  167|\n",
      "|  main-bonus-archive|  143|\n",
      "|   main-rabota-bonus|  139|\n",
      "|    main-news-rabota|  135|\n",
      "|   main-bonus-rabota|  135|\n",
      "|main-archive-inte...|  132|\n",
      "|    main-rabota-news|  130|\n",
      "|main-internet-rabota|  129|\n",
      "|   main-archive-news|  126|\n",
      "|main-rabota-internet|  123|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "step1_filtered = df.withColumn(\n",
    "    \"error_tag\",\n",
    "    F.max(F.when(~F.col(\"event_type\").isin(\"page\", \"event\"), 1).otherwise(0))\n",
    "    .over(Window.partitionBy(\"user_id\", \"session_id\").orderBy(\"timestamp\"))\n",
    ")\n",
    "step1_filtered.show(2)\n",
    "\n",
    "step2_valid_sessions = step1_filtered.filter(F.col(\"error_tag\") == 0).select(\n",
    "    \"user_id\", \"session_id\", \"event_page\", \"timestamp\"\n",
    ").orderBy(\"user_id\", \"session_id\")\n",
    "step2_valid_sessions.show(2)\n",
    "\n",
    "window_spec = Window.partitionBy(\"user_id\", \"session_id\").orderBy(\"timestamp\")\n",
    "step3_numbered = step2_valid_sessions.withColumn(\n",
    "    \"no\", F.row_number().over(window_spec)\n",
    ").select(\"user_id\", \"session_id\", \"event_page\", \"no\").orderBy(\"user_id\", \"session_id\")\n",
    "step3_numbered.show(2)\n",
    "\n",
    "window_spec_prev_event = Window.partitionBy(\"user_id\", \"session_id\").orderBy(\"no\")\n",
    "step4_with_prev_event = step3_numbered.withColumn(\n",
    "    \"prev_event\", F.lag(\"event_page\").over(window_spec_prev_event)\n",
    ")\n",
    "step4_with_prev_event.show(2)\n",
    "\n",
    "step5_distinct_events = step4_with_prev_event.filter(\n",
    "    (F.col(\"event_page\") != F.col(\"prev_event\")) | F.col(\"prev_event\").isNull()\n",
    ").select(\"user_id\", \"session_id\", \"event_page\", \"no\").orderBy(\"user_id\", \"session_id\", \"no\")\n",
    "step5_distinct_events.show(2)\n",
    "\n",
    "step6_routes = step5_distinct_events.groupBy(\"user_id\", \"session_id\").agg(\n",
    "    F.concat_ws(\"-\", F.collect_list(\"event_page\")).alias(\"route\")\n",
    ")\n",
    "step6_routes.show(3)\n",
    "\n",
    "step7 = step6_routes.groupBy(\"route\").count().orderBy(F.desc(\"count\"))\n",
    "step7.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Spark RDD Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------+----------+----------+\n",
      "|user_id|session_id|  event_type|event_page| timestamp|\n",
      "+-------+----------+------------+----------+----------+\n",
      "|    562|       507|        page|      main|1695584127|\n",
      "|    562|       507|       event|      main|1695584134|\n",
      "|    562|       507|       event|      main|1695584144|\n",
      "|    562|       507|       event|      main|1695584147|\n",
      "|    562|       507|wNaxLlerrorU|      main|1695584154|\n",
      "+-------+----------+------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = se.read.format(\"csv\").option(\"header\", \"true\").option(\"sep\", \"\\t\").load(\"hdfs:/data/clickstream.csv\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:============================>                           (16 + 2) / 32]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "filtered_rdd = df.rdd.filter(lambda row: row.event_type in ('page', 'event')).map(\n",
    "    lambda row: Row(user_id=row.user_id, session_id=row.session_id, event_page=row.event_page, timestamp=row.timestamp)\n",
    ")\n",
    "\n",
    "def assign_row_numbers(data):\n",
    "    sorted_data = sorted(data, key=lambda x: x.timestamp)  # Sort by timestamp\n",
    "    return [(Row(user_id=event.user_id, session_id=event.session_id, event_page=event.event_page, no=index + 1))\n",
    "            for index, event in enumerate(sorted_data)]\n",
    "\n",
    "numbered_rdd = (\n",
    "    filtered_rdd\n",
    "    .keyBy(lambda row: (row.user_id, row.session_id))  # Group by (user_id, session_id)\n",
    "    .groupByKey()\n",
    "    .flatMapValues(assign_row_numbers)  # Assign row numbers\n",
    "    .map(lambda x: x[1])\n",
    ")\n",
    "\n",
    "\n",
    "def remove_consecutive_duplicates(events):\n",
    "    sorted_events = sorted(events, key=lambda x: x.no)\n",
    "    result = []\n",
    "    prev_event = None\n",
    "    for event in sorted_events:\n",
    "        if event.event_page != prev_event:\n",
    "            result.append(event)\n",
    "        prev_event = event.event_page\n",
    "    return result\n",
    "\n",
    "distinct_events_rdd = (\n",
    "    numbered_rdd\n",
    "    .keyBy(lambda row: (row.user_id, row.session_id))  \n",
    "    .groupByKey()\n",
    "    .flatMapValues(remove_consecutive_duplicates)  \n",
    "    .map(lambda x: x[1])  \n",
    ")\n",
    "\n",
    "\n",
    "def build_route(events):\n",
    "    sorted_events = sorted(events, key=lambda x: x.no) \n",
    "    route = '-'.join([event.event_page for event in sorted_events])  \n",
    "    return route\n",
    "\n",
    "routes_per_session_rdd = (\n",
    "    distinct_events_rdd\n",
    "    .keyBy(lambda row: (row.user_id, row.session_id)) \n",
    "    .groupByKey()\n",
    "    .mapValues(build_route) \n",
    "    .map(lambda x: x[1])  \n",
    ")\n",
    "\n",
    "\n",
    "routes_count_rdd = (\n",
    "    routes_per_session_rdd\n",
    "    .map(lambda route: (route, 1)) \n",
    "    .reduceByKey(lambda a, b: a + b)  \n",
    "    .sortBy(lambda x: -x[1]) \n",
    ")\n",
    "\n",
    "for route, count in routes_count_rdd.take(30):\n",
    "    print(f\"Route: {route}, Count: {count}\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "week-4-spark-homework"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
